{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras MLP \n",
    "\n",
    "For more information about keras, have a look [here](https://keras.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "%matplotlib notebook\n",
    "# set this to your working directory\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_pickle('sc_cases_cleaned.pkl',compression='gzip')\n",
    "df=df.reset_index(drop=True)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=0.01, # at min 1% of docs\n",
    "                        max_df=.9,  \n",
    "                        max_features=1000,\n",
    "                        stop_words='english',\n",
    "                        ngram_range=(1,3))\n",
    "X = vectorizer.fit_transform(df['opinion_text'])\n",
    "pd.to_pickle(X,'X.pkl')\n",
    "vocab = vectorizer.get_feature_names()\n",
    "pd.to_pickle(vocab,'vocab.pkl')\n",
    "Y = df['x_republican']\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting started with Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential() # create a sequential model\n",
    "model.add(Dense(50, # output neurons in layer       \n",
    "          input_dim=X.shape[1], # number of inputs\n",
    "          activation='relu')) # activation function\n",
    "model.add(Dense(50, activation='relu')) # hidden layer\n",
    "model.add(Dense(1, activation='sigmoid')) # output layer\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a model\n",
    "\n",
    "# Requires graphviz!\n",
    "\n",
    "!pip install pydot\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "dot = model_to_dot(model,\n",
    "                   show_shapes=True,\n",
    "                   show_layer_names=False,\n",
    "                   dpi=70)\n",
    "SVG(dot.create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model.compile(loss='binary_crossentropy', # cost function\n",
    "              optimizer='adam', # use adam as the optimizer\n",
    "              metrics=['accuracy']) # compute accuracy, for scoring\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info = model.fit(X.todense(), Y, \n",
    "                      epochs=5,\n",
    "                      validation_split=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the learned coefficients\n",
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot performance by epoch\n",
    "plt.plot(model_info.epoch,model_info.history['accuracy'])\n",
    "plt.plot(model_info.epoch,model_info.history['val_accuracy'])\n",
    "plt.legend(['train', 'val'], loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# form probability distribution over classes\n",
    "Ypred_prob = model.predict(X.todense())\n",
    "print (Ypred_prob.squeeze()[:5])\n",
    "Ypred = (Ypred_prob > .5).astype(float)\n",
    "print (Ypred.squeeze()[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a model\n",
    "model.save('keras-clf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "from keras.models import load_model\n",
    "model = load_model('keras-clf.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression model with R-squared\n",
    "Yreg = df['log_cite_count']\n",
    "\n",
    "\n",
    "model = Sequential() # create a sequential model\n",
    "model.add(Dense(100, # output neurons in layer       \n",
    "          input_dim=X.shape[1], # number of inputs\n",
    "          activation='relu')) # activation function\n",
    "model.add(Dense(50, activation='relu')) # hidden layer\n",
    "model.add(Dense(1)) # output layer\n",
    "\n",
    "from keras import backend as K\n",
    "def r2(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square( y_true-y_pred )) \n",
    "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) ) \n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
    "\n",
    "model.compile(loss='mean_squared_error', # cost function\n",
    "              optimizer='adam', # use adam as the optimizer\n",
    "              metrics=[r2]) # compute r-squared\n",
    "model_info = model.fit(X.todense(), Yreg, \n",
    "                      epochs=15)\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "Ypred = model.predict(X.todense())\n",
    "\n",
    "print (Yreg[:5], Ypred.squeeze()[:5])\n",
    "r2_score(Yreg,Ypred.squeeze())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Plot performance by epoch\n",
    "plt.plot(model_info.epoch,model_info.history['r2'])\n",
    "plt.legend(['train', 'val'], loc='best')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoders\n",
    "\n",
    "neural nets that perform domain-specific lossy compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential() # create a sequential model\n",
    "model.add(Dense(100, # first compression layer       \n",
    "          input_dim=X.shape[1], # number of inputs\n",
    "          activation='relu')) # activation function\n",
    "model.add(Dense(25, activation='relu', name=\"compression_layer\")) # final compression layer layer\n",
    "model.add(Dense(100, activation='relu')) # first reconstruction layer\n",
    "model.add(Dense(X.shape[1], activation='relu')) # final reconstruction layer\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a model\n",
    "\n",
    "# Requires graphviz\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "dot = model_to_dot(model,\n",
    "                   show_shapes=True,\n",
    "                   show_layer_names=False,\n",
    "                   dpi=70)\n",
    "SVG(dot.create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model.compile(loss='mean_squared_error', # cost function\n",
    "              optimizer='adam', # use adam as the optimizer\n",
    "              metrics=[r2]) # compute accuracy, for scoring\n",
    "\n",
    "model_info = model.fit(X.todense(), X.todense(), \n",
    "                      epochs=10,\n",
    "                      validation_split=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compress the data\n",
    "\n",
    "import keras\n",
    "\n",
    "\n",
    "compression_model = keras.Model(inputs=model.input,\n",
    "                                       outputs=model.get_layer(\"compression_layer\").output)\n",
    "X_compressed = compression_model(X.todense())\n",
    "print (X_compressed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% PCA Viz\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# visualize X and X_compressed\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3,svd_solver='randomized')\n",
    "Xpca = pca.fit_transform(X.todense())\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=Xpca[:,0], y=Xpca[:,1],\n",
    "    hue=Y,\n",
    "    palette=sns.color_palette(\"hls\", len(set(Y))), alpha=0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize X and X_compressed\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3,svd_solver='randomized')\n",
    "Xpca = pca.fit_transform(X_compressed)\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=Xpca[:,0], y=Xpca[:,1],\n",
    "    hue=Y,\n",
    "    palette=sns.color_palette(\"hls\", len(set(Y))), alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Lookup\n",
    "\n",
    "Keras provides functionality to feed just words (actually indices of words) as model input. The model then performs an embedding lookup (we go from sparse one-hot to dense) which then becomes the input for further computation in the model. For a more detailed tutorial, have a look [here](https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/). \n",
    "\n",
    "First, we have to pre-process the data once again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['opinion_text']\n",
    "\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "# tokenize the text\n",
    "\n",
    "tokenized = [text_to_word_sequence(opinion) for opinion in df[\"opinion_text\"]]\n",
    "print (tokenized[0][:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counter = Counter()\n",
    "for i in tokenized:\n",
    "        counter.update(i)\n",
    "print (counter.most_common(10))\n",
    "num_words = len(counter)\n",
    "print (num_words) ## 58'787\n",
    "print (max(len(i) for i in tokenized)) # 26'097, this is one of the challenges of working with legal text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create one_hot representation for each word\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "length_vocab = 10000\n",
    "X_one_hot = [one_hot(opinion, n=length_vocab) for opinion in df[\"opinion_text\"]]\n",
    "print (X_one_hot[0][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next, we pad (or truncate) such that all the inputs have same length\n",
    "\n",
    "max_seq_length = 2000\n",
    "X_one_hot_padded = pad_sequences(X_one_hot, padding='post', maxlen=max_seq_length, truncating='post')\n",
    "X_one_hot_padded.shape # (768, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embedding lookup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "model = Sequential() # create a sequential model\n",
    "model.add(Embedding(length_vocab, 64, input_length=max_seq_length, name=\"embedding_layer\"))\n",
    "model.summary() #640'000 params because 64 dim for 10'000 words\n",
    "\n",
    "# that's it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM in keras\n",
    "\n",
    "Because we have an embedding lookup now, we can train an LSTM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "\n",
    "model = Sequential() # create a sequential model\n",
    "model.add(Embedding(length_vocab, 32, input_length=max_seq_length, name=\"embedding_layer\"))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(32, activation=\"relu\"))\n",
    "model.add(Dense(1, activation=\"sigmoid\")) # output layer\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "dot = model_to_dot(model,\n",
    "                   show_shapes=True,\n",
    "                   show_layer_names=False,\n",
    "                   dpi=70)\n",
    "SVG(dot.create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model.compile(loss='binary_crossentropy', # cost function\n",
    "              optimizer='adam', # use adam as the optimizer\n",
    "              metrics=['accuracy']) # compute accuracy, for scoring\n",
    "\n",
    "model_info = model.fit(X_one_hot_padded, Y, \n",
    "                      epochs=3,\n",
    "                      validation_split=.2, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text Vectorization Layer** <br>\n",
    "more details [here](https://keras.io/api/layers/preprocessing_layers/core_preprocessing_layers/text_vectorization/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.experimental.preprocessing import TextVectorization\n",
    "import tensorflow as tf\n",
    "from keras.layers import LSTM\n",
    "\n",
    "\n",
    "text_dataset = tf.data.Dataset.from_tensor_slices(df[\"opinion_text\"])\n",
    "max_features = 10000  # Maximum vocab size.\n",
    "max_len = 2000  # Sequence length to pad the outputs to.\n",
    "\n",
    "# Create the layer.  \n",
    "vectorize_layer = TextVectorization(\n",
    " max_tokens=max_features,\n",
    " output_mode='int',\n",
    " output_sequence_length=max_len)\n",
    "# Now that the vocab layer has been created, call `adapt` on the text-only  \n",
    "# dataset to create the vocabulary. You don't have to batch, but for large  \n",
    "# datasets this means we're not keeping spare copies of the dataset.\n",
    "\n",
    "\n",
    "vectorize_layer.adapt(text_dataset.batch(64))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "\n",
    "\n",
    "model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
    "model.add(vectorize_layer)\n",
    "model.add(Embedding(max_features, 64, name=\"embedding_layer\"))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dense(1, activation=\"sigmoid\")) # output layer\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy', # cost function\n",
    "              optimizer='adam', # use adam as the optimizer\n",
    "              metrics=['accuracy']) # compute accuracy, for scoring\n",
    "\n",
    "model_info = model.fit(df[\"opinion_text\"], Y, \n",
    "                      epochs=3,\n",
    "                      validation_split=.2, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep learning tips, tricks and advanced features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a basic model again for advanced features.\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense\n",
    "model = Sequential()\n",
    "# set custom activation, specify input dim\n",
    "model.add(Dense(64, input_dim=1000, activation='gelu')) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializers\n",
    "model.add(Dense(64, kernel_initializer='he_normal'))\n",
    "model.add(Dense(64, kernel_initializer='he_uniform'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other activation functions (https://keras.io/activations/)\n",
    "model.add(Dense(64, activation=\"elu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch normalization\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "model.add(Dense(64, use_bias=False)) \n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regularization\n",
    "from keras.regularizers import l1, l2, l1_l2\n",
    "model.add(Dense(64, \n",
    "                kernel_regularizer=l2(0.01),\n",
    "                activity_regularizer=l1(0.01)))\n",
    "model.add(Dense(64, \n",
    "                kernel_regularizer=l1_l2(l1=0.01,l2=.01),\n",
    "                activity_regularizer=l1_l2(l1=0.01,l2=.01)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout\n",
    "from keras.layers import Dropout\n",
    "# np.random.rand(1000)\n",
    "model.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# different loss functions\n",
    "\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping\n",
    "from keras.callbacks import EarlyStopping\n",
    "earlystop = EarlyStopping(monitor='val_accuracy', \n",
    "                          min_delta=0.0001, \n",
    "                          patience=5, \n",
    "                          mode='auto')\n",
    "\n",
    "\n",
    "model.fit(X.todense(), Y, batch_size=128, \n",
    "           epochs=100, \n",
    "           callbacks=[earlystop], \n",
    "           validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Training with Large Data\n",
    "from numpy import memmap\n",
    "X_mm = memmap('X.pkl',shape=(768, 1000))\n",
    "\n",
    "model.fit(X_mm, Y, batch_size=128, \n",
    "           epochs=3, \n",
    "           validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search with KerasClassifier\n",
    "\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# instantiate KerasClassifier with build function\n",
    "def create_model(hidden_layers=1):  \n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_dim=1000, \n",
    "                    activation='relu')) \n",
    "    for i in range(hidden_layers):\n",
    "        model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                optimizer='adam', \n",
    "                metrics= ['accuracy'])\n",
    "    return model\n",
    "\n",
    "clf = KerasClassifier(create_model)\n",
    "\n",
    "# set of grid search CV to select number of hidden layers\n",
    "params = {'hidden_layers' : [0,1,2,3]}\n",
    "grid = GridSearchCV(clf, param_grid=params)\n",
    "grid.fit(X.todense(),Y)\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
